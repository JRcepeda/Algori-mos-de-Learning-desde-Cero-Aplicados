{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\ricardo\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import re\n",
    "from typing import NamedTuple\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import math \n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RICARDO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RICARDO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam:bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea pronosticar con una probabilidad bayessiana P(spam|token), por lo cual aplicando el teorema obtenemos P(token|spam) y P(token|Nospam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrucción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self,k=0.5):\n",
    "        self.k=k\n",
    "        self.tokens=set()# crea un conjunto, que es una coleccion desordena de elementos unicos\n",
    "        self.token_spam_counts=defaultdict(int)#crea diccionarios que tienen claves con valor de cero automatico, su valor cuenta el numero de veces que una palabras esta registra como spam\n",
    "        self.token_ham_counts=defaultdict(int)#el valor de este diccionario cuenta el numero de veces que una palabra se registra como no spam o ham\n",
    "        self.spam_messages=self.ham_messages=0# se inician en cero el nuemro de mensajes de spam y de no spam\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text=text.lower()\n",
    "        all_words=re.findall(\"[a-z0-9]+\",text)#extrae palabras\n",
    "        return set(all_words)#retorna una coleccion donde cada elmento es una palabra\n",
    "        \n",
    "    def fit(self,x_train,y_train):\n",
    "        #recorre cada uno de los registros de los datos de entrenamiento\n",
    "        for i in range(0,y_train.shape[0]):\n",
    "            #incrementa el conteo de si es es spam o no\n",
    "            if y_train[i]==1:\n",
    "                self.spam_messages+=1# cuenta registros de spam\n",
    "            else:\n",
    "                self.ham_messages+=1 #cuenta registros que no son spam\n",
    "            \n",
    "            # incremento de conteo de palabras en el diccionario si pertenecen a spam o no\n",
    "            for token in self.tokenize(x_train[i]):#retorna una coleccion con todas las plabras sin duplciados ej {'ciencia','datos'}\n",
    "                self.tokens.add(token)#añade cada palabra a la coleccion sin importar si es spam o no\n",
    "                \n",
    "                if y_train[i]==1:#revisa si el mensaje en que esta iterando el bucle es spam se se añade a su respectivo diccionario y aumentan la cuenta de sur valor\n",
    "                    self.token_spam_counts[token]+=1 #cuenta tokens de spam\n",
    "                else:\n",
    "                    self.token_ham_counts[token]+=1 #cuenta tokens que no son spam\n",
    "                    \n",
    "     #obtiene las probabilidaes condicionadas a partir de los datos de entrenamiento               \n",
    "    def probabilities(self,token):\n",
    "        spam=self.token_spam_counts[token]#le asigna el valor de conteo que ha tenido un token considerado spam a partir del entramiento\n",
    "        ham=self.token_ham_counts[token]#si no existe asigna un cero\n",
    "        #P(token|spam) \n",
    "        p_token_spam=(spam+self.k)/(self.spam_messages+(2*self.k))# es un parametro en donde se asumen k palabras adicionales de spam y 2k mensajes de spam adicionales  \n",
    "        #P(token|ham)\n",
    "        p_token_ham=(ham+self.k)/(self.ham_messages+(2*self.k))\n",
    "        \n",
    "        return p_token_ham,p_token_spam\n",
    "    \n",
    "    #predice la probabilidad\n",
    "    def predict_prob(self, x_test):\n",
    "        result=[]\n",
    "        for i in range(x_test.shape[0]):\n",
    "            text_tokens=self.tokenize(x_test[i]) #obtiene la coleccion de palabras\n",
    "            log_prob_if_spam=log_prob_if_ham=0.0\n",
    "            \n",
    "            #iterar cada palabra en el vocabulario y asi se van sumando la probabildiaddes logaritmoicas que seria igual que multippicar p(token1|spam)*p(token_i|spam)\n",
    "            for token in self.tokens:\n",
    "                prob_if_ham,prob_if_spam=self.probabilities(token)#obtiene las probabiidad p(token|spam) p(token|ham)\n",
    "                \n",
    "                if token in text_tokens:\n",
    "                    #se suman las probabilidades logaritmicas de cada palabra de un registro dado un evento para el onjunto de prueba\n",
    "                    log_prob_if_spam+=math.log(prob_if_spam)\n",
    "                    log_prob_if_ham+=math.log(prob_if_ham)\n",
    "                \n",
    "                else:\n",
    "                    log_prob_if_spam+=math.log(1.0-prob_if_spam)\n",
    "                    log_prob_if_ham+=math.log(1.0-prob_if_ham)\n",
    "\n",
    "            #ahora sacamos el exponencial del logaritmo para obtener p(token1|spam)*p(token_n|spam)\n",
    "            prob_if_spam=math.exp(log_prob_if_spam)\n",
    "            prob_if_ham=math.exp(log_prob_if_ham)\n",
    "            \n",
    "            #aca agregamos la probabilidad de que una muestra sea spam o no\n",
    "            try:\n",
    "                result.append(prob_if_spam/(prob_if_spam+prob_if_ham))\n",
    "            except:\n",
    "                print('Introduzca mas muestras para entrenamiento')\n",
    "                result.append(0.5)\n",
    "        #ahora retornarmos la probabilidad de que sea spam dado que hay spam y no spam\n",
    "        return np.array(result)\n",
    "         \n",
    "    #predice la etiqueta\n",
    "    def predict(self,x_test):\n",
    "        label=np.where(self.predict_prob(x_test)>0.5,1,0)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text=text.lower()\n",
    "    all_words=re.findall(\"[a-z0-9]+\",text)#extrae palabras\n",
    "    return set(all_words)#retorna una coleccion donde cada elmento es una palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos=pd.read_csv('D:\\Documentos\\Aprendizaje\\PHYTON\\Data Science\\spam.csv',encoding='latin-1')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=datos.rename(columns={'v1':'is_spam','v2':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact=pd.factorize(datos['is_spam'])\n",
    "datos['is_spam']=fact[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['count']=datos['text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza(texto):\n",
    "    texto = re.sub(r\"https?\\://\\S+\", '', texto)\n",
    "    texto = re.sub(r'\\<a href', ' ', texto)\n",
    "    texto = re.sub(r'&amp;', 'and', texto)\n",
    "    texto = re.sub(r'<br />', ' ', texto)\n",
    "    texto = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', texto)\n",
    "    texto = re.sub('\\d','', texto)\n",
    "    texto = re.sub(r\"can\\'t\", \"cannot\", texto)\n",
    "    texto = re.sub(r\"it\\'s\", \"it is\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['text']=datos['text'].apply(lambda x: limpieza(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['stop_count']=datos['text'].apply(lambda x: len([x for x in str(x).split(\" \") if x in stopwords]))\n",
    "datos['text']=datos['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split())) \n",
    "datos['text']=datos['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
    "#datos['text']=[' '.join([spell(i) for i in x.split()]) for x in datos['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RICARDO\\AppData\\Local\\Temp\\ipykernel_10776\\722356309.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  datos['text']=datos['text'].str.replace(especial,\" \")\n"
     ]
    }
   ],
   "source": [
    "especial=r\"[^a-z0-9\\s]\"#borrar los caracteres que no este especificados por eso el ^\n",
    "datos['text']=datos['text'].str.replace(especial,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lema=nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['text']=datos['text'].apply(lambda x: \" \".join([lema.lemmatize(x) for x in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=datos[['text','is_spam']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  is_spam\n",
       "0  go jurong point crazy available bugis n great ...        0\n",
       "1                            ok lar joking wif u oni        0\n",
       "2  free entry wkly comp win fa cup final tkts st ...        1\n",
       "3                u dun say early hor u c already say        0\n",
       "4                nah think go usf life around though        0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=np.array(datos.iloc[0:(int(0.8*3000)),0]),np.array(datos.iloc[0:(int(0.8*3000)),1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test=np.array(datos.iloc[int(0.8*3000):,0]),np.array(datos.iloc[int(0.8*3000):,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes=NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados=bayes.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2760\n",
      "           1       0.98      0.86      0.92       412\n",
      "\n",
      "    accuracy                           0.98      3172\n",
      "   macro avg       0.98      0.93      0.95      3172\n",
      "weighted avg       0.98      0.98      0.98      3172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,resultados))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
